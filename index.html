<!DOCTYPE html>
<html>
  <head>
    <title>Robust Robot Motion Retargeting: Rig Unification and Application to Diverse Robots</title>
    <link rel="icon" type="image/x-icon" href="static/images/rilab_logo.jpg">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Robust Robot Motion Retargeting: Rig Unification and Application to Diverse Robots</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Taemoon Jeong</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Taehyun Byun</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jihoon Kim</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="Fourth AUTHOR PERSONAL LINK" target="_blank">Keunjoon Choi</a><sup>3</sup>,</span>
                      <span class="author-block">
                        <a href="Fifth AUTHOR PERSONAL LINK" target="_blank">Sungpyo Lee</a><sup>4</sup>,</span>
                        <span class="author-block">
                          <a href="Sixth AUTHOR PERSONAL LINK" target="_blank">Jaesung Oh</a><sup>3</sup>,</span>
                          <span class="author-block">
                            <a href="seventh AUTHOR PERSONAL LINK" target="_blank">Omar Darwish</a><sup>5</sup>,</span>
                          <span class="author-block">
                            <a href="Eighth AUTHOR PERSONAL LINK" target="_blank">Joohyung Kim</a><sup>5</sup>,</span>
                            <span class="author-block">
                              <a href="Ninth AUTHOR PERSONAL LINK" target="_blank">Sungjoon Choi</a><sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Korea University<sup>1</sup>, 
                      CINAMON<sup>2</sup>, 
                      Rainbow robotics<sup>3</sup>, 
                      NAVER LABS<sup>4</sup>, 
                      University of Illinois Urbana-Champaign<sup>5</sup>
                      <br>Transactions On Cybernetics [Submitted]</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="100%" height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <p>
        This video is a motion result retargeted from a human motion video to the AMBIDEX robot.
        The AMBIDEX robot performs movements smoothly, ensuring no issues such as collisions occur.
        The video was filmed at the Naver 1784 building.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose a robust and automated pipeline for motion retargeting, enabling the easy generation of feasible motions for diverse humanoids using various types of motion data. Our primary focus is generating feasible and expressive robot movements for human-robot interaction and entertainment applications. To address challenges related to different kinematic configurations in motion data, we unify these configurations into a single predefined rig. The motion trajectory of the unified rig is refined to be feasible on a robot, considering the center of mass and foot contact. The refined trajectory is then retargeted to the robot. Additionally, we consider the physical limitations of robots, including joint angle limits, velocity-acceleration restrictions, and self-collision. We adjust the robot's trajectory to closely follow the source motion. The only manual step in our methodology is initially setting the joints of interest (JOI) of the robot, which are the specific joints that directly correspond to those of the unified rig. Our methodology has been successfully applied to \review{12 robots} in simulation and validated on \review{three real robots}, demonstrating its effectiveness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper Image. -->
      <h2 class="title is-3">Overview</h2>
      <div class="columns">
        <div class="column">
          <div style="text-align: center;"> 
            <img src="static/images/figure2.png" alt="MY ALT TEXT" width="800" height="auto"/>
          </div>
          <p>
            <b>(A)</b> Extracting human skeleton motion from motion capture systems or pose estimations.
            <b>(B)</b> Common-rigging process, which includes optimization-based methods called pre-rigging and post-rigging, aims to unify different human skeleton structures into a single predefined rig, referred to as a common-rig.
            <b>(C)</b> Robot motion retargeting consists of Direction vector-based robot target pose, Robot motion trajectory adjustment, and Post-processing. The process generates feasible real robot motions that preserve the features of the source human motion.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Overview -->

<!-- Common-Rigging Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Common-rigging for motion refinement</h2>
      <div id="results-commonrigging" class="Common-Rigging results-commonrigging">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/common-rigging.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p>
            Common-Rigging is a process that unifies motion data with different skeleton structures into a single, standardized rig.
            In the pre-rigging stage, various human skeletons are retargeted to a predefined common rig.
            This common rig incorporates a rigid body structure and physical properties such as mass and moment of inertia, which facilitate the handling of self-collisions and the refinement of noisy poses.
            The post-rigging stage utilizes the physical properties of the common rig to refine the motion.
            Specifically, it considers feet contact and COM position to correct physically implausible poses and smoothen noisy motions.
            The resulting refined motion becomes a stable and feasible motion suitable for robots.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Common-Rigging Video -->

<!-- Robot motion retargeting Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Flexible motion retargeting to diverse robots</h2>
      <div id="results-mr" class="motion-retargeting results-mr">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/mr.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p>
            Our motion retargeting pipeline adapts the unified motion from the common rig to various robot platforms with different kinematic structures and physical constraints.
            The process begins by defining the joints of interest (JOI) for each robot, which correspond to the joints of the common rig.
            A direction vector-based approach is then employed to compute the target pose in the robot's task space, considering the link lengths of the robot.
            The target pose serves as the input for the inverse kinematics problem, which is solved to obtain the robot's joint angles while satisfying robot-specific constraints such as joint limits and self-collision avoidance.
            The robot's motion trajectory is further optimized to closely follow the reference motion while ensuring smoothness and feasibility.
            This flexible pipeline enables the generation of expressive and realistic motions for a wide range of humanoid robots, as demonstrated by the diverse retargeted robot motions showcased in the accompanying visual materials.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Robot motion retargeting Video  -->

<!-- Real Robot motions Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Real Robot Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="carousel-item">
          <video poster="" id="video1" controls muted width="100%" height="100%">
            <source src="static/videos/result_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="carousel-item">
          <video poster="" id="video2" controls muted width="100%" height="100%">
            <source src="static/videos/JF-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="carousel-description">
        <p>
          To validate the effectiveness and practicality of our motion retargeting approach, we conducted experiments on three real robot platforms: AMBIDEX, THORMANG, and JF2.
          These robots have distinct kinematic structures and physical properties, showcasing the versatility of our pipeline.
          The retargeted motions, which were generated using various motion sources such as MoCap data and pose estimation from RGB videos, were successfully executed on the real robots.
          The experiments demonstrated the ability of our approach to generate smooth, stable, and expressive robot motions that closely resemble the original human motions.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Real Robot motions  -->
  <!-- Initialize Bulma Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var carousels = bulmaCarousel.attach('#results-carousel', {
        slidesToScroll: 1,
        slidesToShow: 1,
        loop: true,
        autoplay: false,  // Turn off autoplay
        autoplaySpeed: 0,
        pauseOnHover: true
      });
    });
  </script>

<!-- Pipeline Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Robust Robot Motion Retargeting Pipeline: From Video Capture to Real-Time Execution</h2>
      <div id="results-pipeline" class="pipeline results-pipeline">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/pipeline.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p>
            This video demonstrates the entire workflow of our robust robot motion retargeting pipeline.
            The process begins with capturing human motion using a video camera.
            The captured video is then fed into our pipeline, where the human pose is estimated using state-of-the-art pose estimation techniques.
            The estimated motion undergoes a common-rigging process to unify the skeleton structure and refine the motion, ensuring its suitability for robot execution.
            The refined motion is then retargeted to the robot platform using our flexible and adaptive retargeting approach, which considers the robot's specific kinematic structure and physical constraints.
            Finally, the retargeted motion is executed on the real robot in real-time, showcasing the smoothness, stability, and expressiveness of the generated motion.
            The seamless integration of the various stages of the pipeline, from video capture to real-time robot execution, highlights the practicality and effectiveness of our approach for real-world applications.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Pipeline Video  -->


<!-- Dance with Robot -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Dance with AMBIDEX</h2>
      <div id="results-pipeline" class="pipeline results-pipeline">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/dance_with.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Dance with Robot  -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/paper.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
