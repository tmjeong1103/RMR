<!DOCTYPE html>
<html>
  <head>
    <title>Robust Robot Motion Retargeting: Rig Unification and Application to Diverse Robots</title>
    <link rel="icon" type="image/x-icon" href="static/images/rilab_logo.jpg">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Robust and Expressive Humanoid Motion Retargeting via Optimization-Based Rig Unification</h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://steadfast-helicona-c82.notion.site/Taemoon-Jeong-69e977daa86b450cb8fa6676c1d287e0?pvs=143" target="_blank">Taemoon Jeong</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://sites.google.com/view/taehyun-byun/home?authuser=0" target="_blank">Taehyun Byun</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jihoon Kim</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="Fourth AUTHOR PERSONAL LINK" target="_blank">Keunjoon Choi</a><sup>3</sup>,</span>
                      <span class="author-block">
                        <a href="Fifth AUTHOR PERSONAL LINK" target="_blank">Jaesung Oh</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href="Sixth AUTHOR PERSONAL LINK" target="_blank">Sungpyo Lee</a><sup>4</sup>,</span>
                          <span class="author-block">
                            <a href="seventh AUTHOR PERSONAL LINK" target="_blank">Omar Darwish</a><sup>5</sup>,</span>
                          <span class="author-block">
                            <a href="Eighth AUTHOR PERSONAL LINK" target="_blank">Joohyung Kim</a><sup>5</sup>,</span>
                            <span class="author-block">
                              <a href="https://sites.google.com/view/sungjoon-choi/home?authuser=0" target="_blank">Sungjoon Choi</a><sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-4 publication-authors">
                    <span class="author-block">Korea University<sup>1</sup>, 
                      CINAMON<sup>2</sup>, 
                      Rainbow robotics<sup>3</sup>, 
                      NAVER LABS<sup>4</sup>,<br>
                      University of Illinois Urbana-Champaign<sup>5</sup>
                      <br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <!-- Affiliations Logos -->
                <div class="is-flex is-justify-content-center is-align-items-center" style="gap: 2rem; flex-wrap: wrap; margin-top: 1rem;">
                  <img src="static/images/ku.png" alt="Korea Univ" style="height: 100px;">
                  <img src="static/images/cinamon.jpg" alt="CINAMON" style="height: 100px;">
                  <img src="static/images/rainbow.png" alt="Rainbow Robotics" style="height: 100px;">
                  <img src="static/images/labs.png" alt="NAVER LABS" style="height: 100px;">
                  <img src="static/images/uiuc.png" alt="UIUC" style="height: 100px;">
                </div>

                  <!-- Conference Name -->
                  <p class="is-size-4" style="margin-top: 0.5em; color: #666;">
                    Accepted to <strong>IROS 2025</strong>
                  </p>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="100%" height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <p class="is-size-5">
        This video is a motion result retargeted from a human motion video to the AMBIDEX robot.
        The AMBIDEX robot performs movements smoothly, ensuring no issues such as collisions occur.
        The video was filmed at the Naver 1784 building.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p class="is-size-5">
            Humanoid robots are increasingly being developed for seamless interaction with humans in diverse domains, yet generating expressive and physically-feasible motions remains a core challenge. 
            We propose a robust and automated pipeline for motion retargeting that enables the generation of natural, stable, and highly expressive motions for a wide variety of humanoid robots using different motion data sources, including noisy pose estimations.
            To ensure robustness, our approach unifies motions from different kinematic structures into a common canonical rig, systematically refines the motion trajectory to address infeasible poses, enforces foot-contact constraints, and enhances stability.
            The retargeted motion is then refined to closely follow the source motion while respecting each robot's physical limits.
            Through extensive experiments on 12 simulated robots and validation on three real robots, we show that our methodology reliably produces expressive upper-body movements with consistent foot contact.
            This work represents an important step towards automating robust and expressive motion generation for humanoid robots, enabling deployment in various real-world scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper Image. -->
      <h2 class="title is-3">Overview</h2>
      <div class="columns">
        <div class="column">
          <div style="text-align: center;"> 
            <img src="static/images/figure2.png" alt="MY ALT TEXT" width="1000" height="auto"/>
          </div>
          <div class="content is-size-5 has-text-justified" style="margin-top: 1rem;">
            <p><strong>(A)</strong> Human motion data is first extracted from motion capture systems or 3D pose estimation methods. These inputs may include high-fidelity MoCap recordings or noisy video-based estimations.</p>
            <p><strong>(B)</strong> The <strong>common-rigging process</strong> converts various human skeleton structures into a single unified rig. This step includes <strong>pre-rigging</strong> (to match proportions and joint constraints) and <strong>post-rigging</strong> (to correct foot placement, align the center of mass, and eliminate self-collisions).</p>
            <p><strong>(C)</strong> The refined motion is then <strong>retargeted to diverse robots</strong> using a direction vector-based approach. Joint trajectories are optimized through inverse kinematics, enforcing robot-specific constraints such as joint limits and collision avoidance. The final result is a physically-feasible robot motion that preserves the expressiveness of the original human movement.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Overview -->

<!-- Common-Rigging Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Common-rigging for motion refinement</h2>
      <div id="results-commonrigging" class="Common-Rigging results-commonrigging">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/common-rigging.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="is-size-5">
            <strong>Common-Rigging</strong> is a critical step that unifies motion data from different skeleton structures into a single, standardized rig.
            In the <strong>Pre-rigging</strong> stage, various human skeletons are retargeted to a predefined common rig using inverse kinematics.
            This rig incorporates a rigid body structure and physical properties such as <strong>mass</strong> and <strong>moment of inertia</strong>, which help handle <strong>self-collisions</strong> and refine noisy poses.
            The <strong>Post-rigging</strong> stage uses these physical properties to refine the motion further—enforcing <strong>foot-ground contact</strong>, aligning the <strong>center of mass (COM)</strong>, and smoothing out physically implausible artifacts.
            As a result, the final trajectory becomes both <strong>physically feasible</strong> and <strong>robot-executable</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Common-Rigging Video -->

<!-- Robot motion retargeting Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Flexible motion retargeting to diverse robots</h2>
      <div id="results-mr" class="motion-retargeting results-mr">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/mr.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="is-size-5">
            Our motion retargeting pipeline adapts the unified motion from the common rig to <strong>various robot platforms</strong> with different kinematic structures and physical constraints.
          </p>
          <p class="is-size-5">
            The process begins by identifying <strong>joints of interest (JOI)</strong> for each robot, which correspond to key joints in the common rig.
            A <strong>direction vector-based approach</strong> is then used to compute the robot’s target pose by scaling directional vectors according to each robot’s link lengths.
          </p>
          <p class="is-size-5">
            The resulting target pose is used as input to an <strong>inverse kinematics (IK)</strong> solver, which computes joint angles that respect <strong>robot-specific constraints</strong> such as joint limits, velocity bounds, and <strong>self-collision avoidance</strong>.
          </p>
          <p class="is-size-5">
            After solving IK, the motion trajectory is further <strong>optimized to track the original motion</strong> while ensuring physical feasibility and trajectory smoothness.
          </p>
          <p class="is-size-5">
            This <strong>flexible and generalizable pipeline</strong> enables the generation of expressive, physically-valid motions across a diverse set of humanoid robots, as demonstrated in the accompanying visual materials.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Robot motion retargeting Video  -->

<!-- Real Robot motions Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Real Robot Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="carousel-item">
          <video poster="" id="video1" controls muted width="100%" height="100%">
            <source src="static/videos/result_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="carousel-item">
          <video poster="" id="video2" controls muted width="100%" height="100%">
            <source src="static/videos/JF-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="carousel-description">
      <p class="is-size-5">
        To evaluate the <strong>effectiveness</strong> and <strong>practical applicability</strong> of our motion retargeting pipeline, we conducted real-world experiments on three humanoid platforms: <strong>AMBIDEX</strong>, <strong>THORMANG</strong>, and <strong>JF2</strong>.
      </p>
      <p class="is-size-5">
        These robots possess <strong>distinct kinematic structures</strong> and <strong>physical characteristics</strong>, such as different joint limits, link geometries, and dynamic properties, thereby validating the <strong>generalizability</strong> of our method.
      </p>
      <p class="is-size-5">
        The retargeted motions—generated from both <strong>high-quality MoCap data</strong> and <strong>noisy 3D pose estimations</strong> from RGB videos—were successfully deployed without additional tuning.
      </p>
      <p class="is-size-5">
        Our experiments confirmed that the proposed approach can produce <strong>smooth</strong>, <strong>stable</strong>, and <strong>expressive upper-body motions</strong> that closely follow the original human movement, even under foot-fixed constraints.
      </p>
      </div>
    </div>
  </div>
</section>
<!-- End Real Robot motions  -->
  <!-- Initialize Bulma Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var carousels = bulmaCarousel.attach('#results-carousel', {
        slidesToScroll: 1,
        slidesToShow: 1,
        loop: true,
        autoplay: false,  // Turn off autoplay
        autoplaySpeed: 0,
        pauseOnHover: true
      });
    });
  </script>

<!-- Pipeline Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Robust Robot Motion Retargeting Pipeline: Real-Time Execution</h2>
      <div id="results-pipeline" class="pipeline results-pipeline">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/pipeline.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="is-size-5">
            This video illustrates the <strong>end-to-end workflow</strong> of our <strong>robust robot motion retargeting pipeline</strong>.
          </p>
          <p class="is-size-5">
            The process begins with capturing human motion using a <strong>video camera</strong>. The recorded footage is processed through a <strong>state-of-the-art 3D pose estimation algorithm</strong>, extracting joint-level motion trajectories from monocular RGB input.
          </p>
          <p class="is-size-5">
            The estimated motion is then passed through the <strong>common-rigging module</strong>, where skeletal inconsistencies are resolved and physical feasibility is enforced, preparing the motion for robot deployment.
          </p>
          <p class="is-size-5">
            Next, the refined motion is <strong>retargeted to a target robot</strong> via our flexible pipeline, which accounts for the robot's unique <strong>kinematic configuration</strong> and <strong>physical constraints</strong>, such as joint limits and collision boundaries.
          </p>
          <p class="is-size-5">
            Finally, the resulting trajectory is <strong>executed in real time</strong> on the robot, demonstrating <strong>smoothness</strong>, <strong>stability</strong>, and <strong>expressiveness</strong> that faithfully reflect the original human motion.
          </p>
          <p class="is-size-5">
            This seamless integration—from human video input to real-world robotic execution—highlights the <strong>practicality and deployability</strong> of our framework in real-time human-robot interaction scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Pipeline Video  -->


<!-- Dance with Robot -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Dance with AMBIDEX</h2>
      <div id="results-pipeline" class="pipeline results-pipeline">
        <div class="item">
          <video controls autoplay muted loop width="100%" height="100%">
            <source src="static/videos/dance_with.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Dance with Robot  -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/paper.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{jeong2025robust,
  title     = {Robust and Expressive Humanoid Motion Retargeting via Optimization-Based Rig Unification},
  author    = {Jeong, Taemoon and Byun, Taehyun and Kim, Jihoon and Choi, Keunjoon and Oh, Jaesung and Lee, Sungpyo and Darwish, Omar and Kim, Joohyung and Choi, Sungjoon},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2025},
  note      = {Accepted}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
